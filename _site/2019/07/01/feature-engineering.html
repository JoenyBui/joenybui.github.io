<h2>Feature Engineering</h2>
<p>01 Jul 2019 - Joeny Bui</p>

<h1 id="feature-engineering">Feature Engineering</h1>

<ul>
  <li>the act of extrating features from raw data and transforming them into formats that are suitable for mL</li>
  <li>data always have measurement noise and missing pieces</li>
  <li>ml involves two mathematical entities: models and features</li>
  <li>frequent characteristics of such as wrong, redundant, or missing</li>
  <li>a feature is not a numerical representation of data</li>
  <li>in ML, we pick not only the model, but also the features - itâ€™s a dobule-jointed lever where one affects the other</li>
</ul>

<h2 id="fancy-trick-with-simple-numbers">Fancy Trick with Simple Numbers</h2>

<ul>
  <li>First question to ask for numeric data is whether the magnitude matters? Positive or negative?</li>
  <li>Scale - min and max</li>
  <li>Models that are smooth functions of input features are sensitive to the scale</li>
  <li>Consider normalizing the features for: k-mean clustering, nearest neighbors methods, radial basis function (RBF) kernels, and anything that uses the Euclidean distance</li>
  <li>Logical functions are not sensitive to input feature scale (binary)</li>
  <li>Models based on space-partitioning trees are not sensitive to scale: decision tree, gradient boosted machines, random forests</li>
  <li></li>
</ul>
